## 排序算法

### 介绍

relevance score算法：计算出索引中文本与搜索文本之间的关联匹配程度

es使用的是term frequency /inverse documnent frequency 算法，简称为TF/IDF算法

* Term frequency：搜索文本中的各个词条在field文本中出现了多少次，次数越多越相关

例子：key : hello world ,doc1:hello,your world is good.  doc2:hllo,how are you ,  明显doc1更相关

* inverse doc frequency 搜索文本各个词条在整个索引的所有文档出现了多少次，出现的次数越多，越不相关

例子：doc1:hello，how are you。doc2: your world is good。doc3:hello ，roose。   虽然都只出现一次，但是world出现更少，所以doc2更相关

* field-lenghth norm ：field长度，field越长，相关度越弱

例子：上述例子，两者出现次数一致，doc3中field越短，所以doc3分数越高

## 正排索引倒排索引

倒排索引：用于搜索

正排索引：

例如doc1:{"name":"jack","age":25},doc2:{"name":"tom","age":28}

> 倒排索引

word doc1 doc2

Jack  存在。 不存在

tom。不存在。存在

> 正排索引

即doc values，用于排序，聚合，过滤操作，其保存在磁盘，如果内存足够，es会自动将其缓存在内存中，性能会很高，内存不足够，os会将其写入到磁盘。

doc name age

doc1 jack 25

doc2 tom 28  

## Query Phase原理

1.搜索请求发送到某一个coordinate node，构建一个priority queue，长度以paging操作from和size为准，默认为10，若from=1万，size=10，则构建10010条数据，发送到index对应的所有primary sahrd(replica shard)

2.coordinate node将请求转换到所有shard，每个shard本地搜索，并构建一个本地的prioriy queue

3.各个shard将自己的pirority queue返回给coordinate node，并构建全局的priority queue，将返回的多个10010条数据，merge成10010条数据，放到coordinate node中的priority queue

所以deep paging搜索若太深，每个shard都要存储10010条数据，所以cpu和内存消耗贼大

> fetch phase阶段

4.coordinate node获取的其实是一堆doc id等信息，之后会发送mget api，去各个shard 上批量一次性获取自己想要的数据

## Query参数详解

1.preference

primar_first:优先将请求发送到该shard，_primary:只将请求打到primary shard中去

local，_only_node:xyz,_prefer_node:xyz,_shards:2,3

bouncing results问题：两个doc排序，field值相同，但是排序不同，每次请求轮询打到不同的replica shard上，页面看到的搜索结果排序都不一样，这就是bouncing resutl，也就是跳跃的结果

解决方案：将轮训改成preference设置成一个字符串，比如user_id，每个user搜索的时候，都使用同一个replica shard上执行，就不会看到bouncing results

2.timeout

限定时间，部分获取到的数据直接返回，避免查询耗时过长

3.routing

doc文档路由，_id路由，使得同一个user对应的数据到一个shard上去

4.search_type

Default:query_then_fetch

dfs_query_then_fetch 可以提升revelance sort精准度

## Scorll查询

> 语法

```
POST test_type/_search?scroll=1m
{
  "query": {
    "match_all": {}
  },
  "sort": [
    "_doc"
  ],
  "size": 3
}


返回结果，具有一个scorll_id
{
"_scroll_id": "FGluY2x1ZGVfY29udGV4dF91dWlkDXF1ZXJ5QW5kRmV0Y2gBFHRFWmhLM3NCOW9lUXpxR1VxR2FjAAAAAAAAAgUWX2ZjaEM4dGRSclc4R3FLV0pFUG01Zw==",
"took": 2,
"timed_out": false,
"_shards": {
"total": 1,
"successful": 1,
"skipped": 0,
"failed": 0
},
"hits": {
"total": {
"value": 5,
"relation": "eq"
},
"max_score": null,
"hits": [
{
"_index": "test_type",
"_type": "_doc",
"_id": "8",
"_score": null,
"_source": {
"test_field": "test varsion3"
},
"sort": [
0
]
}
]
}
}
```

> 通过scorll id查询

```
###注意根据前面的时间进行查询
POST _search/scroll

{
  "scroll": "10m",
  "scroll_id": "FGluY2x1ZGVfY29udGV4dF91dWlkDXF1ZXJ5QW5kRmV0Y2gBFHdrWjdLM3NCOW9lUXpxR1VRR2JnAAAAAAAAAhMWX2ZjaEM4dGRSclc4R3FLV0pFUG01Zw=="
}
```

## 索引操作

```
#创建索引
PUT /my_index/_settings
{
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas": 0
  },
  "mappings": {
    "properties": {
      "my_field": {
        "type": "text"
      }
    }
  }
}
#修改索引
PUT /my_index/_settings
{
  "number_of_replicas": 1
}
###删除索引
DELETE /my_index
DELETE /my_index1，my_index2
DELETE /my_*
DELETE /_all


可以在elasticsearch.yml中配置禁止删除_all索引
比如使用action.destructive_requires_name:true，之后就不能使用_all删除所有索引
```

## 定制分词器

1.默认分词器：

standard

standard tokenizer：以单词边界进行切分

standard token filter：什么都不做

lowercase token filter：将所有字母转换成小写

stop token filter ：（默认禁用），移除停用词，比如a the it等等

2.修改分词器的设置

> 修改分词器

```
PUT /my_index
{
  "settings": {
    "analysis": {  //分词器相关
      "analyzer": { //分词器
        "es_std": {  //把这个分词器起个名字，就启动english停用词，token filter
          "type": "standard",
          "stopwords": "_english_"
        }
      }
    }
  }
}
```

> 查看分词器

```
POST my_index/_analyze
{
  "analyzer": "standard",
  "text": "a dog is in the house"   //拆分成 a dog is in the house
}

POST my_index/_analyze
{
  "analyzer": "es_std",
  "text": "a dog is in the house"   //拆分成 dog house
}
```

### 自定义分词器

```
{
  "settings": {
    "analysis": {
      "char_filter": {  //自定义char_filter规则
        "&_to_and": {
          "type": "mapping",
          "mappings": [
            "&=>and"
          ]
        }
      },
      "filter": {
        "my_stopwords": {
          "type": "stop",
          "stopwords": [
            "the",
            "a"
          ]
        }
      },
      "analyzer": {
        "my_analyzer": {
          "type": "custom",  //这里是自定义
          "char_filter": [  //单个单词转换，大小写转换，&转换成and
            "html_strip",
            "&_to_and"
          ],
          "tokenizer": "standard",  //以什么方式进行切分，默认单词
          "filter": [   //开启大小写转换（默认只有这个）
            "lowercase",
            "my_stopwords"
          ],
          "stopwords":"_english_"  //移除起停词，a，the，is等
        }
      }
    }
  }
}
```

### 对type使用分词器

```
PUT /my_index/_mapping/my_type
{
  "properties": {
    "content": {
      "type": "text",
      "analyzer": "my_analyzer"
    }
  }
}
```

## type底层

type是一个index中用来区分类似的数据，其可能有不同的fields，而且有不同的属性控制索引建立、分词器

field的value，在底层的lucene中建立索引，全都是spaque bytes类型，不区分类型的

一个index中的多个type，实际上是放在一起存储的，因此一个index下，不能有多个type重名，而类型或者其他设置不同的，因为那样是没法处理的。

```
{
  "ecommerce": {
    "mappings": {
      "a_goods": {
        "properties": {
          "name": {
            "type": "text"
          },
          "price": {
            "type": "double"
          }
        }
      },
      "b_goods": {
        "properties": {
          "name": {
            "type": "text"
          },
          "sale": {
            "type": "double"
          }
        }
      }
    }
  }
}
假设有这么两个type，如果存储在同一个索引，那么a商品最后存储的是
{
  "ecommerce": {
    "mappings": {
      "_type": {
        "type": "text",
        "index": "not_analyzed"
      },
      "name": {
        "type": "text"
      },
      "price": {
        "type": "double"
      },
      "sale": {
        "type": "double"
      }
    }
  }
}
{
"_type":"a",
"name":"a",
"price":"11",
"sale":""  //空值，存在严重性能问题
}
```

## root Object深入剖析

#### 1.root object

就是对应的mapping json，包括了properties，metadata(\_id,_\_source,\_type)，settings(analyzer)，其他settings,详见2.增删改查中settings

#### 2.propetties

参考2.增删改查中settings

#### 3._source

* 查询的时候，可以拿到完整的document，不需要先拿到doc id，再发送一次请求拿doc
* partial update基于_source实现
* reindex时，基于_source实现，不需要查询数据再修改
* 可以基于_source定制返回field
* debug query更容易实现，因为可以直接看到_source
* 可以禁止_source

PUT /index

{"mappings":{"_source":{"enabled":false}}}

#### 4._all

设置索引时，将所有field打包在一起，作为一个_all field，建立索引，没指定任何field进行搜索时，就使用\_al field搜索l

PUT /index

{"_all":{"enabled":false}}

也可以在field级别设置include_all field，设置是否将field的值包含在\_all field中

PUT /index

{"properties":{"fieldName":{"type":"text","include_in_all":false}}}

#### 5.标志性metadata

\_index,\_type,\_id

## 定制化dynamic mapping

> 设置索引的dynammic mapping

```
PUT /index
{
  "mappings": {
    "dynamic": "strict/true/false", // 报错/dynamic mapping可以进行操作/忽略
    "properties": {
      "title": {
        "type": "text"
      },
      "address": {
        "type": "object",
        "dynamic": true
      }
    }
  }
}
```

> 添加content字段返回报错

```
PUT my_index/_doc/1
{
  "title": "my article",
  "content": "this my article",
  "address": {
    "province": "guangdong",
    "city": "Guangzhou"
  }
}

mapping set to strict, dynamic introduction of [content] within [_doc] is not allowed"

如果去除content就没有问题
```

> 查看mapping

```
GET http://192.168.0.120:9200/my_index/_mapping/

GET http://192.168.0.120:9200/my_index/_mapping/

HTTP/1.1 200 OK
content-type: application/json; charset=UTF-8

{
  "my_index": {
    "mappings": {
      "dynamic": "strict",  //这里是starict
      "properties": {
        "stash": {
          "dynamic": "true",   //这里设置成的是true
          "properties": {
            "city": {
              "type": "text",
              "fields": {
                "keyword": {
                  "type": "keyword",
                  "ignore_above": 256
                }
              }
            },
            "province": {
              "type": "text",
              "fields": {
                "keyword": {
                  "type": "keyword",
                  "ignore_above": 256
                }
              }
            }
          }
        },
        "title": {
          "type": "text"
        }
      }
    }
  }
}

Response code: 200 (OK); Time: 117ms; Content length: 295 bytes

```

> 自定义dynamic template

就是自定义模版，比如匹配到_en则使用english模版，会停用is ，a停用分词器

```
PUT /index
{
  "mappings": {
    "dynamic_templates": [
      {
        "en": {
          "match": "*_en",
          "match_mapping_type": "string",
          "mapping": {
            "type": "text",
            "analyzer": "english"
          }
        }
      }
    ]
  }
}
###放入两条数据
PUT /index/_doc/2 
{
  "title_en": "this is my first article"   以及title
}
###查找数据方式一
POST /index/_serach
{
  "query": {
    "match": {
      "title_en": "is"   //查找不到
    }
  }
}
###方式二，能否通过query dsl的方式查找匹配is的数据呢
POST /my_index5/_search?q=is   //只会返回带title字段的数据
```

> 自定义自己的default mapping template

```
{
  "mapings": {
    "_default": {
      "_all": {"enables":false}   //遇见不认识字段忽略
    },
    "fieldName":{"_all":"true"}   //遇见不认识字段dynamic mapping
  }
}
```

